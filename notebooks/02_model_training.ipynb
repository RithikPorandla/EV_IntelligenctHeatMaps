{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Training - EV Charging Demand Prediction\n",
    "\n",
    "**MA EV ChargeMap Portfolio Project**\n",
    "\n",
    "This notebook trains a machine learning model to predict daily charging demand (kWh) for candidate EV charging sites.\n",
    "\n",
    "## Goals\n",
    "1. Prepare features and target variable\n",
    "2. Train multiple regression models\n",
    "3. Evaluate model performance\n",
    "4. Analyze feature importance\n",
    "5. Export trained model for API deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../backend')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "from app.config import settings\n",
    "\n",
    "engine = create_engine(settings.database_url)\n",
    "\n",
    "# Load sites data\n",
    "df = pd.read_sql(\"SELECT * FROM sites WHERE city = 'worcester'\", engine)\n",
    "\n",
    "print(f\"Loaded {len(df)} sites\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "feature_cols = [\n",
    "    'traffic_index',\n",
    "    'pop_density_index',\n",
    "    'renters_share',\n",
    "    'income_index',\n",
    "    'poi_index',\n",
    "    'parking_lot_flag',\n",
    "    'municipal_parcel_flag'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target_col = 'daily_kwh_estimate'\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Min: {y.min():.1f} kWh\")\n",
    "print(f\"  Max: {y.max():.1f} kWh\")\n",
    "print(f\"  Mean: {y.mean():.1f} kWh\")\n",
    "print(f\"  Std: {y.std():.1f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Multiple Models\n",
    "\n",
    "We'll compare three regression algorithms:\n",
    "1. Linear Regression (baseline)\n",
    "2. Random Forest Regressor\n",
    "3. Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train R²: {train_r2:.4f}\")\n",
    "    print(f\"  Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.2f} kWh\")\n",
    "    print(f\"  Test MAE: {test_mae:.2f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train R²': [r['train_r2'] for r in results.values()],\n",
    "    'Test R²': [r['test_r2'] for r in results.values()],\n",
    "    'Test RMSE': [r['test_rmse'] for r in results.values()],\n",
    "    'Test MAE': [r['test_mae'] for r in results.values()],\n",
    "})\n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R² scores\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Test R²'], color='steelblue')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('Model Comparison: R² Score (Higher is Better)')\n",
    "axes[0].set_ylim([0, 1])\n",
    "for i, v in enumerate(comparison_df['Test R²']):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "# RMSE\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['Test RMSE'], color='coral')\n",
    "axes[1].set_ylabel('RMSE (kWh)')\n",
    "axes[1].set_title('Model Comparison: RMSE (Lower is Better)')\n",
    "for i, v in enumerate(comparison_df['Test RMSE']):\n",
    "    axes[1].text(i, v + 1, f'{v:.1f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Best Model\n",
    "\n",
    "We'll select the model with the best test R² score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on test R²\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['test_r2'])\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Test R²: {results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"Test RMSE: {results[best_model_name]['test_rmse']:.2f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of predictions vs actual\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, y_pred_best, alpha=0.6, s=50)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Daily kWh')\n",
    "plt.ylabel('Predicted Daily kWh')\n",
    "plt.title(f'{best_model_name}: Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred_best\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_best, residuals, alpha=0.6, s=50)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Daily kWh')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance\n",
    "\n",
    "For tree-based models, we can analyze which features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Create dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n=== Feature Importance ===\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'{best_model_name}: Feature Importance', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\n{best_model_name} does not provide feature importances.\")\n",
    "    print(\"For linear models, check coefficients instead.\")\n",
    "    \n",
    "    if hasattr(best_model, 'coef_'):\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Coefficient': best_model.coef_\n",
    "        }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "        \n",
    "        print(\"\\n=== Model Coefficients ===\")\n",
    "        print(coef_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation\n",
    "\n",
    "Verify model stability with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation on best model\n",
    "print(f\"Running 5-fold cross-validation for {best_model_name}...\\n\")\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, \n",
    "                           scoring='r2', n_jobs=-1)\n",
    "\n",
    "print(f\"Cross-validation R² scores: {cv_scores}\")\n",
    "print(f\"Mean CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Visualize CV scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(1, 6), cv_scores, color='steelblue', alpha=0.7)\n",
    "plt.axhline(cv_scores.mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {cv_scores.mean():.3f}')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Cross-Validation Scores', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full dataset\n",
    "print(\"Training final model on full dataset...\")\n",
    "final_model = type(best_model)(**best_model.get_params())\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Export model\n",
    "model_path = '../models/site_score_model.pkl'\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(f\"\\n✓ Model saved to {model_path}\")\n",
    "print(f\"\\nModel details:\")\n",
    "print(f\"  Type: {type(final_model).__name__}\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "print(f\"  Training samples: {len(X)}\")\n",
    "print(f\"  Expected R² on new data: ~{cv_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Testing\n",
    "\n",
    "Test the exported model to ensure it loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exported model\n",
    "with open(model_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Test prediction\n",
    "test_features = np.array([[\n",
    "    0.7,  # traffic_index\n",
    "    0.6,  # pop_density_index\n",
    "    0.5,  # renters_share\n",
    "    0.4,  # income_index\n",
    "    0.65, # poi_index\n",
    "    1,    # parking_lot_flag\n",
    "    0     # municipal_parcel_flag\n",
    "]])\n",
    "\n",
    "prediction = loaded_model.predict(test_features)[0]\n",
    "\n",
    "print(f\"\\n=== Model Test ===\")\n",
    "print(f\"Test input features: {test_features[0]}\")\n",
    "print(f\"Predicted daily kWh: {prediction:.1f}\")\n",
    "print(f\"\\n✓ Model loads and predicts successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "We trained and compared three regression models for predicting daily EV charging demand:\n",
    "\n",
    "1. **Linear Regression**: Baseline model\n",
    "2. **Random Forest**: Ensemble tree-based model\n",
    "3. **Gradient Boosting**: Advanced ensemble model\n",
    "\n",
    "The best model was selected based on test set R² score and exported for deployment.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "The most important features for predicting charging demand are:\n",
    "- Traffic index\n",
    "- Population density\n",
    "- Points of interest index\n",
    "\n",
    "### Deployment\n",
    "\n",
    "The trained model is saved as a pickle file and can be loaded by the FastAPI backend to provide real-time predictions via the `/api/predict` endpoint.\n",
    "\n",
    "### Limitations & Future Work\n",
    "\n",
    "1. **Data**: Model trained on synthetic/simulated data. In production, would use real charger usage data.\n",
    "2. **Features**: Could add temporal features (time of day, day of week) for better predictions.\n",
    "3. **Validation**: With real data, could validate against actual charger utilization.\n",
    "4. **Model Updates**: Should retrain periodically as new data becomes available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
