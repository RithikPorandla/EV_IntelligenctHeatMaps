# Data Pipeline Documentation

This document describes the data processing pipeline for MA EV ChargeMap.

---

## Overview

The data pipeline transforms raw data (or synthetic data for the portfolio) into scored candidate EV charging sites stored in a PostgreSQL database.

### Pipeline Goals

1. **Reproducibility**: Same input ‚Üí same output
2. **Modularity**: Each step is independent and reusable
3. **Clarity**: Well-documented with clear data flows
4. **Testability**: Each script can run independently

---

## Pipeline Architecture

```
Raw Data Sources
       ‚Üì
   [Ingest Scripts]
       ‚Üì
  Feature Tables
       ‚Üì
  [Score Computation]
       ‚Üì
    Sites Table
       ‚Üì
   API / Frontend
```

---

## Directory Structure

```
data/
‚îú‚îÄ‚îÄ raw/                    # Raw data files (manually downloaded)
‚îÇ   ‚îú‚îÄ‚îÄ worcester_parcels.geojson
‚îÇ   ‚îú‚îÄ‚îÄ demographics.csv
‚îÇ   ‚îî‚îÄ‚îÄ traffic_counts.csv
‚îÇ
‚îú‚îÄ‚îÄ processed/              # Cleaned/transformed data
‚îÇ   ‚îî‚îÄ‚îÄ [Generated by scripts]
‚îÇ
‚îú‚îÄ‚îÄ ingest_parcels.py       # Step 1: Candidate sites
‚îú‚îÄ‚îÄ ingest_demographics.py  # Step 2: Demographic features
‚îú‚îÄ‚îÄ ingest_traffic.py       # Step 3: Traffic features
‚îú‚îÄ‚îÄ build_scores.py         # Step 4: Score computation
‚îî‚îÄ‚îÄ run_pipeline.sh         # Master script to run all steps
```

---

## Pipeline Steps

### Step 0: Database Setup

**Purpose**: Initialize PostgreSQL database and tables

**Script**: `backend/app/database.py`

**Actions**:
1. Create database (if not exists)
2. Create `sites` table via SQLAlchemy
3. Create indexes on frequently queried columns

**Run**:
```bash
cd backend
python -c "from app.database import init_db; init_db()"
```

**Schema**:
```sql
CREATE TABLE sites (
    id SERIAL PRIMARY KEY,
    city VARCHAR NOT NULL,
    lat FLOAT NOT NULL,
    lng FLOAT NOT NULL,
    location_label VARCHAR,
    parcel_id VARCHAR,
    
    -- Features (0-1 normalized)
    traffic_index FLOAT DEFAULT 0,
    pop_density_index FLOAT DEFAULT 0,
    renters_share FLOAT DEFAULT 0,
    income_index FLOAT DEFAULT 0,
    poi_index FLOAT DEFAULT 0,
    parking_lot_flag INTEGER DEFAULT 0,
    municipal_parcel_flag INTEGER DEFAULT 0,
    
    -- Scores (0-100)
    score_demand FLOAT NOT NULL,
    score_equity FLOAT NOT NULL,
    score_traffic FLOAT NOT NULL,
    score_grid FLOAT NOT NULL,
    score_overall FLOAT NOT NULL,
    
    -- Predictions
    daily_kwh_estimate FLOAT NOT NULL
);

CREATE INDEX idx_city_score ON sites(city, score_overall);
CREATE INDEX idx_location ON sites(lat, lng);
```

---

### Step 1: Ingest Parcels / Generate Sites

**Script**: `data/ingest_parcels.py`

**Purpose**: Create candidate site locations

**Inputs**:
- Worcester bounding box (hardcoded)
- Grid spacing parameter

**Outputs**:
- `sites` table populated with basic location info
- Fields: id, city, lat, lng, location_label, parcel_id

**Logic**:
```python
1. Define Worcester bounding box
2. Generate regular grid of points (lat, lng)
3. For each point:
   - Assign location label (quadrant-based)
   - Generate parcel ID
   - Insert into database
4. Initialize all feature/score fields to 0
```

**Run**:
```bash
cd data
python ingest_parcels.py
```

**Output**:
```
üó∫Ô∏è  Generating Worcester candidate sites...
Clearing existing Worcester sites...
Generating grid points...
Generated 542 candidate locations
Inserting sites into database...
‚úì Inserted 542 sites
‚úì Parcel ingestion complete
```

**Key Functions**:
- `generate_grid_points()`: Creates regular grid
- `generate_location_labels()`: Simple quadrant naming

**Production Changes**:
```python
# Load actual Worcester parcels
import geopandas as gpd
parcels = gpd.read_file('data/raw/worcester_parcels.geojson')

# Filter for suitable parcel types
commercial = parcels[parcels['use_code'].isin(['COM', 'MUN', 'PARK'])]

# Compute centroids
sites = commercial.copy()
sites['lat'] = sites.geometry.centroid.y
sites['lng'] = sites.geometry.centroid.x
```

---

### Step 2: Ingest Demographics

**Script**: `data/ingest_demographics.py`

**Purpose**: Add demographic features to each site

**Inputs**:
- Sites from database (lat, lng)
- Demographic data (real or synthetic)

**Outputs**:
- Updates `sites` table with:
  - `pop_density_index`
  - `income_index`
  - `renters_share`
  - `poi_index`
  - `parking_lot_flag`
  - `municipal_parcel_flag`

**Logic** (Synthetic Version):
```python
1. Load all Worcester sites from database
2. For each site:
   - Compute distance to downtown
   - Generate pop_density_index (higher near center)
   - Generate income_index (spatial pattern)
   - Generate renters_share (correlated with density, income)
   - Generate poi_index (correlated with density)
   - Generate parking_lot_flag (probabilistic)
   - Generate municipal_parcel_flag (probabilistic)
3. Update site in database
4. Commit all changes
```

**Run**:
```bash
cd data
python ingest_demographics.py
```

**Output**:
```
üë• Generating demographic features for Worcester sites...
Processing 542 sites...
  Processed 100/542 sites
  Processed 200/542 sites
  ...
Saving to database...
‚úì Updated 542 sites with demographic features
‚úì Demographics ingestion complete
```

**Key Functions**:
- `generate_pop_density_index()`: Density gradient from downtown
- `generate_income_index()`: Income by neighborhood
- `generate_renters_share()`: Renter fraction
- `generate_poi_index()`: Activity/amenity density

**Production Changes**:
```python
# Load MAPC demographics by census tract
demographics = pd.read_csv('data/raw/mapc_demographics.csv')

# Load census tract boundaries
tracts = gpd.read_file('data/raw/ma_census_tracts.geojson')

# Spatial join sites to tracts
sites_gdf = gpd.GeoDataFrame(
    sites, 
    geometry=gpd.points_from_xy(sites.lng, sites.lat),
    crs='EPSG:4326'
)
sites_joined = gpd.sjoin(sites_gdf, tracts, how='left')

# Merge demographic data
sites_final = sites_joined.merge(demographics, on='tract_id')

# Normalize features to 0-1
sites_final['pop_density_index'] = (
    (sites_final['pop_density'] - sites_final['pop_density'].min()) /
    (sites_final['pop_density'].max() - sites_final['pop_density'].min())
)
```

---

### Step 3: Ingest Traffic

**Script**: `data/ingest_traffic.py`

**Purpose**: Add traffic features to each site

**Inputs**:
- Sites from database (lat, lng)
- Traffic count data (real or synthetic)

**Outputs**:
- Updates `sites` table with:
  - `traffic_index`

**Logic** (Synthetic Version):
```python
1. Define major traffic corridors (I-290, Route 9, etc.)
2. Load all Worcester sites from database
3. For each site:
   - Compute distance to downtown
   - Compute distance to each major corridor
   - Combine into traffic index (higher near corridors)
   - Add random noise for variation
4. Update site in database
5. Commit all changes
```

**Run**:
```bash
cd data
python ingest_traffic.py
```

**Output**:
```
üöó Generating traffic features for Worcester sites...
Processing 542 sites...
  Processed 100/542 sites
  Processed 200/542 sites
  ...
Saving to database...
‚úì Updated 542 sites with traffic features
‚úì Traffic ingestion complete
```

**Key Functions**:
- `generate_traffic_index()`: Traffic based on proximity to corridors
- `distance_to_point()`: Haversine distance helper

**Production Changes**:
```python
# Load MassDOT traffic counts
traffic = gpd.read_file('data/raw/massdot_traffic_2023.geojson')

# For each site, find nearest traffic count point
from scipy.spatial import cKDTree

traffic_points = np.array(list(zip(traffic.lng, traffic.lat)))
tree = cKDTree(traffic_points)

for site in sites:
    # Find nearest traffic count
    dist, idx = tree.query([site.lng, site.lat])
    nearest_aadt = traffic.iloc[idx]['aadt']
    
    # Normalize to 0-1
    site.traffic_index = min(nearest_aadt / 50000, 1.0)
```

---

### Step 4: Build Scores

**Script**: `data/build_scores.py`

**Purpose**: Compute final scores for all sites

**Inputs**:
- Sites from database (with all features populated)
- Scoring formulas from `backend/app/services/scoring.py`

**Outputs**:
- Updates `sites` table with:
  - `score_demand`
  - `score_equity`
  - `score_traffic`
  - `score_grid`
  - `score_overall`
  - `daily_kwh_estimate`

**Logic**:
```python
1. Load all Worcester sites from database
2. Check that features are populated
3. For each site:
   - Prepare features dictionary
   - Call ScoringService.compute_all_scores()
   - Update site with computed scores
4. Commit all changes
5. Print summary statistics and top 10 sites
```

**Run**:
```bash
cd data
python build_scores.py
```

**Output**:
```
üìä Computing scores for Worcester sites...
Processing 542 sites...
  Processed 100/542 sites
  Processed 200/542 sites
  ...
Saving to database...

üìà Score Summary Statistics:
  Total sites: 542

  Overall Score:
    Min:  18.5
    Max:  91.2
    Mean: 54.3

  Daily kWh Estimate:
    Total: 145,820 kWh/day
    Mean:  269.1 kWh/day per site

  üèÜ Top 10 Sites by Overall Score:
    1. Worcester Central-East (Grid): 91.2 (425.0 kWh/day)
    2. Worcester North-East (Grid): 87.5 (398.2 kWh/day)
    ...

‚úì Score computation complete
```

**Key Functions**:
Uses `ScoringService` from `backend/app/services/scoring.py`:
- `compute_demand_score()`
- `compute_equity_score()`
- `compute_traffic_score()`
- `compute_grid_score()`
- `compute_overall_score()`
- `estimate_daily_kwh()`

---

## Running the Full Pipeline

### Option 1: Shell Script

```bash
cd data
./run_pipeline.sh
```

The script:
1. Activates Python virtual environment
2. Runs all 4 steps in sequence
3. Prints summary after completion

### Option 2: Manual Execution

```bash
cd data

python ingest_parcels.py
python ingest_demographics.py
python ingest_traffic.py
python build_scores.py
```

### Option 3: Docker

```bash
# From project root
docker-compose exec backend bash

# Inside container
cd /app
python ../data/ingest_parcels.py
python ../data/ingest_demographics.py
python ../data/ingest_traffic.py
python ../data/build_scores.py
```

---

## Data Quality Checks

### Automated Checks (in scripts)

1. **Coordinate Bounds**:
   ```python
   assert 42.0 < lat < 43.0  # Massachusetts latitude range
   assert -72.0 < lng < -71.0  # Worcester longitude range
   ```

2. **Feature Ranges**:
   ```python
   assert 0.0 <= traffic_index <= 1.0
   assert 0.0 <= score_overall <= 100.0
   ```

3. **Required Fields**:
   ```python
   assert site.lat is not None
   assert site.lng is not None
   ```

### Manual Validation

After pipeline completion:

```sql
-- Check for nulls
SELECT COUNT(*) FROM sites WHERE traffic_index IS NULL;

-- Check score ranges
SELECT MIN(score_overall), MAX(score_overall) FROM sites;

-- Check feature distributions
SELECT 
  AVG(traffic_index), 
  STDDEV(traffic_index)
FROM sites;

-- Find potential outliers
SELECT * FROM sites 
WHERE score_overall < 10 OR score_overall > 95
ORDER BY score_overall DESC;
```

---

## Performance

### Current Performance (500-1000 sites)

- **Parcel ingestion**: ~2 seconds
- **Demographics**: ~5 seconds
- **Traffic**: ~5 seconds
- **Score computation**: ~3 seconds
- **Total pipeline**: ~15 seconds

### Optimization Strategies

1. **Batch Inserts**: Use `bulk_save_objects()` instead of individual inserts
2. **Vectorization**: Use NumPy for array operations
3. **Parallel Processing**: Process sites in parallel with `multiprocessing`
4. **Caching**: Cache distance calculations

**Example Parallelization**:
```python
from multiprocessing import Pool

def process_site(site):
    # Compute features for one site
    return updated_site

with Pool(4) as pool:
    updated_sites = pool.map(process_site, sites)
```

---

## Error Handling

### Common Issues

1. **Database Connection Failed**
   - Check PostgreSQL is running
   - Verify DATABASE_URL in .env
   - Ensure database exists

2. **No Sites Found**
   - Run `ingest_parcels.py` first
   - Check database table exists

3. **Feature Values Out of Range**
   - Check normalization logic
   - Verify input data quality

### Logging

Scripts print progress to stdout:
```python
print("Processing 542 sites...")
if (idx + 1) % 100 == 0:
    print(f"  Processed {idx + 1}/{len(sites)} sites")
```

Production would use Python `logging` module:
```python
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"Processing {len(sites)} sites...")
logger.warning("Site missing traffic data, using default")
logger.error("Database connection failed")
```

---

## Testing

### Unit Tests

```python
# test_pipeline.py
import pytest
from data.ingest_demographics import generate_pop_density_index

def test_pop_density_index():
    # Downtown should have high density
    downtown_density = generate_pop_density_index(42.2626, -71.8023)
    assert downtown_density > 0.7
    
    # Outskirts should have lower density
    outskirt_density = generate_pop_density_index(42.31, -71.87)
    assert outskirt_density < 0.5
```

### Integration Tests

```python
def test_full_pipeline():
    # Run full pipeline
    from data.ingest_parcels import main as ingest_parcels
    from data.build_scores import main as build_scores
    
    ingest_parcels()
    # ... other steps
    build_scores()
    
    # Verify output
    sites = session.query(Site).all()
    assert len(sites) > 0
    assert all(0 <= s.score_overall <= 100 for s in sites)
```

---

## Future Enhancements

### Data Source Integration

1. **Automated Downloads**
   ```python
   import requests
   
   def download_parcels():
       url = "https://opendata.worcesterma.gov/api/..."
       response = requests.get(url)
       with open('data/raw/parcels.geojson', 'wb') as f:
           f.write(response.content)
   ```

2. **API Integration**
   ```python
   def fetch_census_data(tract_ids):
       api_key = os.getenv('CENSUS_API_KEY')
       url = f"https://api.census.gov/data/..."
       # Fetch and process
   ```

### Incremental Updates

Instead of full rebuild:
```python
def update_site(site_id):
    # Update features for one site
    # Recompute scores
    # More efficient for small changes
```

### Data Versioning

Track pipeline runs:
```python
CREATE TABLE pipeline_runs (
    id SERIAL PRIMARY KEY,
    run_date TIMESTAMP,
    sites_processed INTEGER,
    mean_score FLOAT,
    data_sources JSONB
);
```

---

## Monitoring

### Metrics to Track

- Pipeline execution time
- Number of sites processed
- Score distributions (mean, std dev)
- Data quality flags (nulls, outliers)

### Alerts

- Pipeline failure
- Unusual score distributions
- Database errors

---

## Documentation

Each script includes:
- Docstring explaining purpose
- Comments on key logic
- Usage examples
- Data source references

Example:
```python
"""
Parcel data ingestion for Worcester, MA.

This script generates candidate EV charging site locations based on a grid
over Worcester. In a production system, this would load actual parcel data
from MassGIS or Worcester open data portal.

Data source (for reference):
- Worcester parcel polygons: https://opendata.worcesterma.gov/datasets/...

For this portfolio project, we create a synthetic grid of candidate locations.
"""
```

---

## Summary

The MA EV ChargeMap data pipeline:
1. ‚úÖ Modular: Each step is independent
2. ‚úÖ Reproducible: Same input ‚Üí same output
3. ‚úÖ Documented: Clear explanations and references
4. ‚úÖ Testable: Scripts can run independently
5. ‚úÖ Production-ready structure: Easy to swap synthetic ‚Üí real data

**Next Steps**: Run notebooks (`01_eda_worcester.ipynb`, `02_model_training.ipynb`) to analyze pipeline output and train ML models.
